#model_dir
model_name: "meta-llama/Llama-3.1-8B"
hypernetwork_ckpt: /disk/liuxb/code/Multi-EMoE/{}_{}_hypernetwork.pth
retrieval_model_ckpt: "facebook/contriever-msmarco"
decompose_model_ckpt: /disk/liuxb/code/Multi-EMoE/Decomposer/falcon3-1b-{}

#dataset
data_name: WikiMhQA
pre_train_datasets: /disk/liuxb/code/Multi-EMoE/datasets/pre_training.json
train_dataset: /disk/liuxb/code/Multi-EMoE/datasets/{}_train.json
test_dataset: /disk/liuxb/code/Multi-EMoE/datasets/{}_test.json

#model
half: False
train_batch_size: 1
valid_batch_size: 1

#train
learning_rate: 1e-4
deberta_lr: 1e-5
learning_rate_min: 1e-6
single_layer: [1] #Inserting an expert layer

#parameter
embed_feature: 4096
in_feature: 4096
hid_feature: 4096
out_feature: 4096
rank: 512

#hyper-parameter
k: 1